{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ef1f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a893a070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14c15385",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/phi-2\"\n",
    "output_dir = \"./phi2-qlora-finetuned-med/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d036622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure QLoRA parameters\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea7940c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95008572615b4b93bbdd9f0c0792068f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the base model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2708f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e0e2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                       # Rank\n",
    "    lora_alpha=16,             # Alpha parameter\n",
    "    lora_dropout=0.1,          # Dropout probability\n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],  # Verify these match Phi-2's architecture\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "834f3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for QLoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "824f5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_csv(\"final_dataset.csv\")\n",
    "dataset_dict = dataset.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c300cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset_dict['train'],\n",
    "    'validation': dataset_dict['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5296ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation_pairs(hf_dataset):\n",
    "    \"\"\"\n",
    "    Given a HuggingFace Dataset with columns 'Question' and 'Answer',\n",
    "    returns a list of dicts with keys 'instruction' and 'output'.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for example in hf_dataset:\n",
    "        pairs.append({\n",
    "            \"instruction\": example[\"input\"],\n",
    "            \"output\": example[\"output\"]\n",
    "        })\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3219e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs: 76047\n",
      "First pair: {'instruction': 'What is suggested by the presence of a new murmur after a prosthetic valve has been implanted?', 'output': 'The presence of a new murmur after a prosthetic valve has been implanted is suggestive of prosthetic valve dysfunction. A new murmur can indicate that there is a problem with the valve, such as a leak or a blockage, which can lead to reduced blood flow and other complications. In some cases, additional testing, such as an echocardiogram or cardiac catheterization, may be needed to confirm the diagnosis and determine the best course of treatment.'}\n"
     ]
    }
   ],
   "source": [
    "conversation_pairs = create_conversation_pairs(dataset_dict['train'])\n",
    "print(f\"Number of pairs: {len(conversation_pairs)}\")\n",
    "print(\"First pair:\", conversation_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a91831c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs: 8450\n",
      "First pair: {'instruction': 'What is subacute sclerosing panencephalitis (SSPE), and what causes it?', 'output': 'Subacute sclerosing panencephalitis (SSPE) is a rare, progressive, and usually fatal neurological disorder that occurs as a result of persistent infection of the brain by the measles virus. The virus causes inflammation and damage to the brain, leading to symptoms such as seizures, dementia, and loss of motor function. SSPE typically develops several years after a person has had measles, and is more common in individuals who contracted the virus at a young age. There is currently no cure for SSPE, and treatment is mainly focused on managing the symptoms.'}\n"
     ]
    }
   ],
   "source": [
    "conversation_pairs_validation = create_conversation_pairs(dataset_dict['validation'])\n",
    "print(f\"Number of pairs: {len(conversation_pairs_validation)}\")\n",
    "print(\"First pair:\", conversation_pairs_validation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7054349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(conversation_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c00099af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Format: <|user|>instruction<|assistant|>output<|endoftext|>\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    \n",
    "    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"]):\n",
    "        # Create the prompt\n",
    "        prompt = f\"<|user|>{instruction}<|assistant|>{output}<|endoftext|>\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Create labels (same as input_ids for causal LM)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c4a2272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416e2de96c39436e9f7adfc1c070b4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/76047 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"output\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64c6ad85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320e2f26acdc43c492a1664e892bbabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_tokenized = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"output\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "060e93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6551a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a596f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a234e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4d9ba3687c471f934d7e09f86b9450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4aac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "peft_model_path = os.path.join(output_dir, \"peft_model\")\n",
    "model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "from peft import PeftConfig\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "config.save_pretrained(peft_model_path)\n",
    "\n",
    "print(f\"Model saved to {peft_model_path}\")\n",
    "print(f\"Tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inference\n",
    "def generate_response(instruction, model, tokenizer, max_length=512):\n",
    "    prompt = f\"<|user|>{instruction}<|assistant|>\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # Extract only the assistant's response\n",
    "    response = response.split(\"<|assistant|>\")[1].split(\"<|endoftext|>\")[0].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with a sample question\n",
    "test_instruction = \" how long will 6 mgs of suboxone block opiates?\"\n",
    "response = generate_response(test_instruction, model, tokenizer)\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
